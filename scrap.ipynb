{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class RecursiveWebScraper:\n",
    "    def __init__(self, base_url, delay=1, max_depth=2, skip_domains=None):\n",
    "        self.base_url = base_url\n",
    "        self.delay = delay\n",
    "        self.max_depth = max_depth\n",
    "        self.visited_urls = set()\n",
    "        self.domain = urlparse(base_url).netloc\n",
    "        self.skip_domains = skip_domains or []  # Domains to skip\n",
    "        self.results = {}\n",
    "        self.url_file = \"To_scrape.txt\"\n",
    "        self.url_done = \"Scraped.txt\"\n",
    "\n",
    "    def scrape(self, url, depth):\n",
    "        if depth == 0:\n",
    "            return  # Stop recursion when depth limit is reached\n",
    "        \n",
    "        if url in self.visited_urls:\n",
    "            return  # Avoid redundant visits\n",
    "\n",
    "        print(f\"Scraping: {url} | Depth: {self.max_depth - depth + 1}\")\n",
    "        self.visited_urls.add(url)\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Failed to fetch {url}: {e}\")\n",
    "            return\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        self.results[url] = self.format_content_as_markdown(url, soup)\n",
    "        with open(self.url_done, \"a+\") as file:\n",
    "            file.write(f\"{url}\\n\")\n",
    "\n",
    "        # Extract and process links\n",
    "        for link_tag in soup.find_all('a', href=True):\n",
    "            full_url = urljoin(url, link_tag['href'])\n",
    "            if self.is_valid_url(full_url):\n",
    "                time.sleep(self.delay)  # Respectful scraping\n",
    "                self.scrape(full_url, depth - 1)  # Recurse with reduced depth\n",
    "\n",
    "    def is_valid_url(self, url):\n",
    "        \"\"\"Check if the URL is valid, within the same domain, and not in the skip list.\"\"\"\n",
    "        parsed_url = urlparse(url)\n",
    "        if parsed_url.netloc != self.domain:\n",
    "            return False\n",
    "        for skip_domain in self.skip_domains:\n",
    "            if url.startswith(skip_domain):\n",
    "                return False\n",
    "        return url not in self.visited_urls\n",
    "\n",
    "    def format_content_as_markdown(self, url, soup):\n",
    "        \"\"\"Extract content and format it as Markdown with inline links.\"\"\"\n",
    "        markdown = [f\"# {soup.title.string.strip() if soup.title else url}\"]\n",
    "\n",
    "        # Traverse the content and replace links inline\n",
    "        for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'ul', 'li']):\n",
    "            if tag.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "                level = int(tag.name[1])\n",
    "                markdown.append(f\"{'#' * level} {self.process_inline_links(tag)}\")\n",
    "            elif tag.name == 'p':\n",
    "                markdown.append(self.process_inline_links(tag))\n",
    "            elif tag.name == 'ul':\n",
    "                for li in tag.find_all('li'):\n",
    "                    markdown.append(f\"- {self.process_inline_links(li)}\")\n",
    "\n",
    "        return \"\\n\\n\".join(markdown)\n",
    "\n",
    "    def process_inline_links(self, tag):\n",
    "        \"\"\"Replace links inline within a tag.\"\"\"\n",
    "        text = \"\"\n",
    "        for content in tag.contents:\n",
    "            if content.name == 'a' and content.get('href'):\n",
    "                link_text = content.get_text(strip=True)\n",
    "                href = content['href']\n",
    "                full_url = urljoin(self.base_url, href)\n",
    "                with open(self.url_file, \"a+\") as file:\n",
    "                    file.write(f\"{full_url}\\n\")\n",
    "                text += f\"[{link_text}]({full_url})\"\n",
    "            elif isinstance(content, str):\n",
    "                text += content\n",
    "        return text.strip()\n",
    "\n",
    "    def save_results(self, filename=\"scraped_data.md\"):\n",
    "        \"\"\"Save the scraped results to a Markdown file.\"\"\"\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "            for url, content in self.results.items():\n",
    "                file.write(f\"URL: {url}\\n\\n{content}\\n\\n{'-' * 80}\\n\\n\")\n",
    "        print(f\"Scraped data saved to {filename}\")\n",
    "\n",
    "    def results_df(self):\n",
    "        \"\"\"Return the scraped results as a Pandas DataFrame.\"\"\"\n",
    "        return pd.DataFrame(self.results.items(), columns=[\"URL\", \"Content\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://soliditylang.org/about/ | Depth: 1\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'full_url' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 9\u001b[0m\n\u001b[1;32m      3\u001b[0m skip_domains \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://soliditylang.org/blog\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# \"https://docs.soliditylang.org/en\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m ]\n\u001b[1;32m      8\u001b[0m scraper \u001b[38;5;241m=\u001b[39m RecursiveWebScraper(base_url, delay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, skip_domains\u001b[38;5;241m=\u001b[39mskip_domains)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mscraper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscrape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Start scraping with depth limit of 2\u001b[39;00m\n\u001b[1;32m     10\u001b[0m scraper\u001b[38;5;241m.\u001b[39msave_results(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscraped_data.md\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[37], line 45\u001b[0m, in \u001b[0;36mRecursiveWebScraper.scrape\u001b[0;34m(self, url, depth)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     44\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults[url] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_content_as_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msoup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl_done, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma+\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     47\u001b[0m     file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[37], line 74\u001b[0m, in \u001b[0;36mRecursiveWebScraper.format_content_as_markdown\u001b[0;34m(self, url, soup)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tag\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh5\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh6\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     73\u001b[0m     level \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(tag\u001b[38;5;241m.\u001b[39mname[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 74\u001b[0m     markdown\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mlevel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_inline_links\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tag\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     76\u001b[0m     markdown\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_inline_links(tag))\n",
      "Cell \u001b[0;32mIn[37], line 95\u001b[0m, in \u001b[0;36mRecursiveWebScraper.process_inline_links\u001b[0;34m(self, tag)\u001b[0m\n\u001b[1;32m     93\u001b[0m         text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m content\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma+\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m---> 95\u001b[0m     file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mfull_url\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'full_url' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "base_url = \"https://soliditylang.org/about/\"\n",
    "skip_domains = [\n",
    "    \"https://soliditylang.org/blog\",\n",
    "    # \"https://docs.soliditylang.org/en\"\n",
    "]\n",
    "\n",
    "scraper = RecursiveWebScraper(base_url, delay=1, max_depth=2, skip_domains=skip_domains)\n",
    "scraper.scrape(base_url, depth=2)  # Start scraping with depth limit of 2\n",
    "scraper.save_results(\"scraped_data.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = scraper.df_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyano",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
